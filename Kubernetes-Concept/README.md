<div align="center">

# â˜¸ï¸ Kubernetes - Container Orchestration

### *Automate Deployment, Scaling, and Management of Containerized Applications*

[![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white)](https://kubernetes.io)
[![CNCF](https://img.shields.io/badge/CNCF-Graduated-00ADD8?style=for-the-badge&logo=cncf&logoColor=white)](https://www.cncf.io/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://docker.com)

---

*Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications.*

</div>

---

## ğŸ“‘ Table of Contents

- [Core Concepts](#-core-concepts)
- [Architecture](#-architecture)
- [Kubernetes Components](#-kubernetes-components)
- [Pods](#-pods)
- [ReplicaSets & Replication Controllers](#-replicasets--replication-controllers)
- [Deployments](#-deployments)
- [Networking](#-networking)
- [Services](#-services)
- [Kubernetes in Cloud](#-kubernetes-in-cloud)
- [YAML Manifests](#-yaml-manifests)
- [Commands Reference](#-commands-reference)

---

## ğŸ¯ Core Concepts

### The Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       KUBERNETES OBJECT HIERARCHY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                          DEPLOYMENT                                  â”‚  â”‚
â”‚   â”‚    (Manages rollouts, rollbacks, scaling)                           â”‚  â”‚
â”‚   â”‚                              â”‚                                       â”‚  â”‚
â”‚   â”‚                              â–¼                                       â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚   â”‚   â”‚                      REPLICASET                              â”‚   â”‚  â”‚
â”‚   â”‚   â”‚    (Ensures desired number of pod replicas)                  â”‚   â”‚  â”‚
â”‚   â”‚   â”‚                          â”‚                                   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚                          â–¼                                   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚                      PODS                            â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚    (Smallest deployable unit)                        â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚                      â”‚                               â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚                      â–¼                               â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   â”‚              CONTAINERS                      â”‚   â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   â”‚    (Running Docker images)                   â”‚   â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   Image â”€â”€â–º Container â”€â”€â–º Pod â”€â”€â–º ReplicaSet â”€â”€â–º Deployment                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Terminology

| Term | Description |
|:-----|:------------|
| **Node** | A VM or physical machine (like EC2 instance) |
| **Pod** | Smallest deployable unit containing containers |
| **Cluster** | A set of nodes working together |
| **Master** | Node that manages the cluster |
| **Worker** | Node that runs application workloads |

---

## ğŸ—ï¸ Architecture

### Cluster Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          KUBERNETES CLUSTER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MASTER NODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                        (Control Plane)                               â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚   â”‚   â”‚    API     â”‚  â”‚    etcd    â”‚  â”‚ Controller â”‚  â”‚  Scheduler â”‚   â”‚  â”‚
â”‚   â”‚   â”‚   Server   â”‚  â”‚  (Store)   â”‚  â”‚  Manager   â”‚  â”‚            â”‚   â”‚  â”‚
â”‚   â”‚   â”‚            â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚   â”‚  â”‚
â”‚   â”‚   â”‚ â€¢ Frontend â”‚  â”‚ â€¢ Key-Val  â”‚  â”‚ â€¢ Brain    â”‚  â”‚ â€¢ Assigns  â”‚   â”‚  â”‚
â”‚   â”‚   â”‚ â€¢ REST API â”‚  â”‚ â€¢ Cluster  â”‚  â”‚ â€¢ Watches  â”‚  â”‚   pods to  â”‚   â”‚  â”‚
â”‚   â”‚   â”‚ â€¢ Auth     â”‚  â”‚   state    â”‚  â”‚ â€¢ Responds â”‚  â”‚   nodes    â”‚   â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â”‚ kubectl commands                       â”‚
â”‚                                    â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WORKER NODES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚   â”‚   â”‚   Worker Node 1  â”‚   â”‚   Worker Node 2  â”‚   â”‚  Worker Node 3 â”‚  â”‚  â”‚
â”‚   â”‚   â”‚                  â”‚   â”‚                  â”‚   â”‚                â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚   Kubelet    â”‚ â”‚   â”‚ â”‚   Kubelet    â”‚ â”‚   â”‚ â”‚  Kubelet   â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚   (Agent)    â”‚ â”‚   â”‚ â”‚   (Agent)    â”‚ â”‚   â”‚ â”‚  (Agent)   â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚  Container   â”‚ â”‚   â”‚ â”‚  Container   â”‚ â”‚   â”‚ â”‚ Container  â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚   Runtime    â”‚ â”‚   â”‚ â”‚   Runtime    â”‚ â”‚   â”‚ â”‚  Runtime   â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚   (Docker)   â”‚ â”‚   â”‚ â”‚   (Docker)   â”‚ â”‚   â”‚ â”‚  (Docker)  â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚    Pods      â”‚ â”‚   â”‚ â”‚    Pods      â”‚ â”‚   â”‚ â”‚   Pods     â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â”‚  [App] [App] â”‚ â”‚   â”‚ â”‚  [App] [DB]  â”‚ â”‚   â”‚ â”‚ [App][App] â”‚ â”‚  â”‚  â”‚
â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Kubernetes Components

### Control Plane (Master) Components

<table>
<tr>
<td width="50%">

**ğŸŒ API Server**

The frontend for Kubernetes. All communications go through it.

```
Users, CLI, UI
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server â”‚ â—„â”€â”€ Entry point for all
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     REST operations
```

- Handles all REST requests
- Authenticates users
- Validates requests
- Updates etcd

</td>
<td width="50%">

**ğŸ’¾ etcd**

Distributed key-value store for all cluster data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         etcd            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Key   â”‚   Value   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ node1 â”‚ {...}     â”‚  â”‚
â”‚  â”‚ pod1  â”‚ {...}     â”‚  â”‚
â”‚  â”‚ svc1  â”‚ {...}     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Stores cluster state
- Implements locks
- Prevents conflicts

</td>
</tr>
<tr>
<td>

**ğŸ§  Controller Manager**

The brain of Kubernetes orchestration.

- Monitors cluster state
- Responds when nodes/containers fail
- Makes decisions to bring up new containers
- Ensures desired state matches actual state

```
Desired: 3 pods â”€â”€â–º Running: 2 pods
                         â”‚
            Controller notices difference
                         â”‚
                         â–¼
              Creates 1 more pod
```

</td>
<td>

**ğŸ“… Scheduler**

Assigns pods to nodes based on resource requirements.

- Watches for newly created pods
- Selects optimal node for each pod
- Considers:
  - Resource requirements
  - Hardware constraints
  - Affinity/anti-affinity
  - Data locality

```
New Pod â”€â”€â–º Scheduler â”€â”€â–º Best Node
```

</td>
</tr>
</table>

### Worker Node Components

| Component | Description |
|:----------|:------------|
| **Kubelet** | Agent on each node; ensures containers run as expected |
| **Container Runtime** | Software to run containers (Docker, containerd, CRI-O) |
| **Kube-proxy** | Network proxy; maintains network rules on nodes |

### kubectl - The Command Line Tool

```bash
# kubectl = Kubernetes Control
# Used to deploy and manage applications on a K8s cluster

kubectl cluster-info          # View cluster information
kubectl get nodes             # List all nodes in cluster
kubectl run <name> --image=   # Deploy an application
```

---

## ğŸ“¦ Pods

> *Pod is the smallest deployable unit in Kubernetes â€” a single instance of an application.*

### Pod Concept

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              POD STRUCTURE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                              POD                                     â”‚  â”‚
â”‚   â”‚                         (10.244.0.2)                                â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚  â”‚
â”‚   â”‚   â”‚   Container 1   â”‚        â”‚   Container 2   â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚   (App)         â”‚ â—„â”€â”€â”€â”€â–º â”‚   (Helper)      â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚                 â”‚  same  â”‚                 â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚   nginx:1.21    â”‚network â”‚   log-agent     â”‚                   â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â€¢ Share same network namespace (localhost communication)          â”‚  â”‚
â”‚   â”‚   â€¢ Share same storage volumes                                      â”‚  â”‚
â”‚   â”‚   â€¢ Same lifecycle (created/destroyed together)                     â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   âš ï¸  Multiple containers OK, but NOT of same type (no app + app)          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Points

- **Pods don't deploy containers directly** â€” Kubernetes encapsulates containers in pods
- **1:1 relationship** typical between pod and main container
- **Multi-container pods** allowed for helper containers (sidecar pattern)
- **Same-type containers** in one pod are NOT allowed

### Pod YAML Structure

```yaml
# pod-definition.yaml
apiVersion: v1              # API version
kind: Pod                   # Type of object
metadata:                   # Metadata about the object
  name: myapp-pod
  labels:
    app: myapp
    tier: frontend
spec:                       # Specification/desired state
  containers:
    - name: nginx-container
      image: nginx:1.21
      ports:
        - containerPort: 80
```

### Pod Commands

```bash
# Create pod from YAML
kubectl create -f pod-definition.yaml
kubectl apply -f pod-definition.yaml

# List pods
kubectl get pods
kubectl get pods -o wide          # More details

# Describe pod
kubectl describe pod <pod-name>

# Delete pod
kubectl delete pod <pod-name>

# Generate YAML template
kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml

# Edit running pod
kubectl edit pod <pod-name>
```

---

## ğŸ”„ ReplicaSets & Replication Controllers

> *Ensure high availability by maintaining a specified number of pod replicas running at all times.*

### Why Replicas?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WHY REPLICATION?                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   SCENARIO 1: High Availability                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  Pod 1  â”‚ â—„â”€â”€ If this fails     â”‚  Pod 1  â”‚  â”‚  Pod 2  â”‚              â”‚
â”‚   â”‚  (App)  â”‚                       â”‚  (App)  â”‚  â”‚  (App)  â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       âŒ                                âœ…           âœ…                     â”‚
â”‚   User sees error!                  User still served!                     â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO 2: Load Balancing & Scaling                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                         CLUSTER                                   â”‚    â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚   â”‚   â”‚     Node 1        â”‚         â”‚     Node 2        â”‚            â”‚    â”‚
â”‚   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚            â”‚    â”‚
â”‚   â”‚   â”‚  â”‚Pod 1â”‚ â”‚Pod 2â”‚  â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  â”‚Pod 3â”‚ â”‚Pod 4â”‚  â”‚            â”‚    â”‚
â”‚   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚   Load  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚            â”‚    â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Balance â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚   ReplicaSet spans across nodes for load distribution!                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ReplicationController vs ReplicaSet

| Feature | ReplicationController | ReplicaSet |
|:--------|:---------------------|:-----------|
| **Status** | âš ï¸ Legacy (deprecated) | âœ… Current |
| **API Version** | `v1` | `apps/v1` |
| **Selector** | Equality-based only | Set-based (more powerful) |
| **Label Matching** | Basic | Advanced (`matchLabels`, `matchExpressions`) |

### ReplicaSet YAML

```yaml
# replicaset-definition.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
spec:
  replicas: 3                    # Desired number of pods
  selector:                      # How to identify pods
    matchLabels:
      app: myapp
  template:                      # Pod template
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx
          image: nginx:1.21
```

### Scaling ReplicaSets

```bash
# Method 1: Update YAML and apply
kubectl replace -f replicaset.yaml

# Method 2: Scale command with file
kubectl scale --replicas=6 -f replicaset.yaml

# Method 3: Scale command with name
kubectl scale --replicas=6 replicaset myapp-replicaset

# Method 4: Edit directly
kubectl edit replicaset myapp-replicaset
```

### ReplicaSet Commands

```bash
kubectl get replicaset              # List ReplicaSets
kubectl get rs                      # Short form
kubectl describe replicaset <name>  # Details
kubectl delete replicaset <name>    # Delete
kubectl explain replicaset          # Documentation
```

---

## ğŸš€ Deployments

> *Deployments provide declarative updates for Pods and ReplicaSets with rollout and rollback capabilities.*

### Why Deployments?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DEPLOYMENT CAPABILITIES                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   âœ… Rolling Updates      - Update pods one-after-another (zero downtime)  â”‚
â”‚   âœ… Rollbacks            - Undo changes if something goes wrong           â”‚
â”‚   âœ… Pause & Resume       - Batch multiple changes together                â”‚
â”‚   âœ… Scaling              - Increase/decrease replicas                     â”‚
â”‚   âœ… Revision History     - Track all deployment versions                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DEPLOYMENT STRATEGIES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   RECREATE (Not Recommended)                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚                                                                             â”‚
â”‚   v1: [Pod][Pod][Pod]  â”€â”€â–º  [   DOWNTIME   ]  â”€â”€â–º  v2: [Pod][Pod][Pod]    â”‚
â”‚                                   âŒ                                        â”‚
â”‚   â€¢ Delete ALL old pods first                                              â”‚
â”‚   â€¢ Then create ALL new pods                                               â”‚
â”‚   â€¢ Application has downtime!                                              â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   ROLLING UPDATE (Default - Recommended)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚                                                                             â”‚
â”‚   v1: [Pod][Pod][Pod]                                                      â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚       [Pod][Pod][v2 Pod]     â† Create new, keep old running               â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚       [Pod][v2 Pod][v2 Pod]  â† Gradually replace                          â”‚
â”‚            â”‚                                                                â”‚
â”‚            â–¼                                                                â”‚
â”‚   v2: [v2 Pod][v2 Pod][v2 Pod]  âœ… Zero downtime!                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Rolling Update Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ROLLING UPDATE INTERNALS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   DEPLOYMENT                                                                â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â”œâ”€â”€â–º OLD ReplicaSet (v1)          NEW ReplicaSet (v2)                â”‚
â”‚       â”‚    replicas: 3 â†’ 2 â†’ 1 â†’ 0      replicas: 0 â†’ 1 â†’ 2 â†’ 3           â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â”‚    [Pod v1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º deleted             â”‚
â”‚       â”‚    [Pod v1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º deleted                     â”‚
â”‚       â”‚    [Pod v1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º deleted                             â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â”‚                                           [Pod v2] â—„â”€â”€â”€ created    â”‚
â”‚       â”‚                                  [Pod v2] â—„â”€â”€â”€ created             â”‚
â”‚       â”‚                         [Pod v2] â—„â”€â”€â”€ created                      â”‚
â”‚       â”‚                                                                     â”‚
â”‚   âš¡ Important: Creates NEW ReplicaSet automatically!                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment YAML

```yaml
# deployment-definition.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx
          image: nginx:1.21
          ports:
            - containerPort: 80
```

### Deployment Commands

```bash
# Create deployment
kubectl create -f deployment.yaml
kubectl apply -f deployment.yaml

# Quick create
kubectl create deployment myapp --image=nginx --replicas=3

# View deployments
kubectl get deployments
kubectl get deploy

# Rollout status
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment

# Update image
kubectl set image deployment/myapp-deployment nginx=nginx:1.22

# Rollback
kubectl rollout undo deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment --to-revision=2

# Describe
kubectl describe deployment myapp-deployment
```

---

## ğŸŒ Networking

### Pod Networking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         POD NETWORKING                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   SINGLE NODE                                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                     Node (192.168.1.2)                               â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   Internal Network: 10.244.0.0/24                                   â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â”‚
â”‚   â”‚   â”‚   Pod 1   â”‚   â”‚   Pod 2   â”‚   â”‚   Pod 3   â”‚                    â”‚  â”‚
â”‚   â”‚   â”‚10.244.0.2 â”‚â—„â”€â–ºâ”‚10.244.0.3 â”‚â—„â”€â–ºâ”‚10.244.0.4 â”‚                    â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚   â”‚         â”‚               â”‚               â”‚                           â”‚  â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚  â”‚
â”‚   â”‚                         â”‚                                            â”‚  â”‚
â”‚   â”‚                  All pods can communicate                            â”‚  â”‚
â”‚   â”‚                  via internal IPs                                    â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Node Networking Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-NODE NETWORKING                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   PROBLEM: Same IP range on different nodes causes conflicts!              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  Node 1             â”‚         â”‚  Node 2             â”‚                  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚  â”‚ Pod: 10.244.â”‚    â”‚   âŒ    â”‚    â”‚ Pod: 10.244.â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚      0.2    â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚      0.2    â”‚  â”‚ Conflict!        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â”‚   SOLUTION: CNI Plugins (Network Solutions)                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  Node 1             â”‚         â”‚  Node 2             â”‚                  â”‚
â”‚   â”‚  Network: 10.244.1.xâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Network: 10.244.2.xâ”‚                  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  CNI    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚  â”‚ Pod: 10.244.â”‚    â”‚ Plugin  â”‚    â”‚ Pod: 10.244.â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚      1.2    â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚      2.2    â”‚  â”‚ âœ… Works!        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â”‚   Popular CNI Plugins: Calico, Flannel, Weave Net, Cilium                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Networking Requirements

- âœ… All pods can communicate with each other **without NAT**
- âœ… All nodes can communicate with all pods **without NAT**
- âœ… The IP a pod sees itself as is the same IP others see it as

---

## ğŸ”Œ Services

> *Services enable communication between various components within and outside of the application.*

### Why Services?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERVICE USE CASES                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚   USERS     â”‚                                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â–¼                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  Frontend   â”‚â”€â”€â”€â”€â”€â–ºâ”‚   Backend   â”‚â”€â”€â”€â”€â”€â–ºâ”‚  Database   â”‚               â”‚
â”‚   â”‚  Service    â”‚      â”‚   Service   â”‚      â”‚   Service   â”‚               â”‚
â”‚   â”‚             â”‚      â”‚             â”‚      â”‚             â”‚               â”‚
â”‚   â”‚  [Pod][Pod] â”‚      â”‚  [Pod][Pod] â”‚      â”‚    [Pod]    â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                             â”‚
â”‚   Services provide:                                                        â”‚
â”‚   â€¢ Stable IP/DNS for pod access                                           â”‚
â”‚   â€¢ Load balancing across pods                                             â”‚
â”‚   â€¢ Service discovery                                                      â”‚
â”‚   â€¢ Loose coupling between components                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Types

#### 1. NodePort

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              NODEPORT                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   External Access â”€â”€â–º Node IP:NodePort â”€â”€â–º Service â”€â”€â–º Pod                 â”‚
â”‚                                                                             â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   Laptop              â”‚            NODE                  â”‚                 â”‚
â”‚   (192.168.1.10)      â”‚        (192.168.1.2)            â”‚                 â”‚
â”‚         â”‚             â”‚                                  â”‚                 â”‚
â”‚         â”‚  Request    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                 â”‚
â”‚         â”‚             â”‚    â”‚       SERVICE           â”‚  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â–ºâ”‚    ClusterIP: 10.96.x   â”‚  â”‚                 â”‚
â”‚        :30008         â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚                 â”‚
â”‚      (NodePort)       â”‚    â”‚    â”‚   Port: 80    â”‚â”€â”€â”€â”€â”‚â”€â”€â”‚â”€â”€â–º Pod:80       â”‚
â”‚                       â”‚    â”‚    â”‚ TargetPort:80 â”‚    â”‚  â”‚   (10.244.0.2)  â”‚
â”‚                       â”‚    â”‚    â”‚ NodePort:30008â”‚    â”‚  â”‚                 â”‚
â”‚                       â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚                 â”‚
â”‚                       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                 â”‚
â”‚                       â”‚                                  â”‚                 â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                             â”‚
â”‚   NodePort Range: 30000 - 32767                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. ClusterIP (Default)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             CLUSTERIP                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Internal cluster communication only (no external access)                 â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                         CLUSTER                                      â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   Frontend Pods              Backend Service              DB Pods    â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”       â”‚  â”‚
â”‚   â”‚   â”‚ Pod â”‚ â”‚ Pod â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ ClusterIP   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Pod â”‚       â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜           â”‚ 10.96.0.50  â”‚           â””â”€â”€â”€â”€â”€â”˜       â”‚  â”‚
â”‚   â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â”‚   â€¢ Pods access backend via ClusterIP (stable endpoint)             â”‚  â”‚
â”‚   â”‚   â€¢ No need to track individual pod IPs                             â”‚  â”‚
â”‚   â”‚   â€¢ Service handles load balancing                                  â”‚  â”‚
â”‚   â”‚                                                                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. LoadBalancer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            LOADBALANCER                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   For cloud providers (AWS, GCP, Azure)                                    â”‚
â”‚                                                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚   Internet â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Cloud Load        â”‚                                 â”‚
â”‚                    â”‚   Balancer          â”‚                                 â”‚
â”‚                    â”‚   (External IP)     â”‚                                 â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                               â”‚                                             â”‚
â”‚                               â–¼                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚                â”‚                â”‚                           â”‚
â”‚              â–¼                â–¼                â–¼                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚ Node 1 â”‚       â”‚ Node 2 â”‚       â”‚ Node 3 â”‚                       â”‚
â”‚         â”‚ [Pod]  â”‚       â”‚ [Pod]  â”‚       â”‚ [Pod]  â”‚                       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”‚   Provisions cloud provider's native load balancer                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service YAML

```yaml
# NodePort Service
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
    - port: 80          # Service port
      targetPort: 80    # Pod port
      nodePort: 30008   # External port (30000-32767)
```

```yaml
# ClusterIP Service (default)
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
    - port: 80
      targetPort: 8080
```

---

## â˜ï¸ Kubernetes in Cloud

### Deployment Options

| Type | Description | Examples |
|:-----|:------------|:---------|
| **Self-Hosted** | You provision & manage VMs, deploy cluster | kops, kubeadm, KubeOne |
| **Managed (K8s-as-a-Service)** | Provider manages control plane | GKE, EKS, AKS |

### Managed Kubernetes Services

| Provider | Service | Description |
|:---------|:--------|:------------|
| **Google Cloud** | GKE | Google Kubernetes Engine |
| **AWS** | EKS | Elastic Kubernetes Service |
| **Azure** | AKS | Azure Kubernetes Service |
| **IBM** | IKS | IBM Kubernetes Service |

### Local Development Options

| Tool | Description |
|:-----|:------------|
| **Minikube** | Single-node cluster in VM; great for learning |
| **Kind** | Kubernetes in Docker; for testing |
| **MicroK8s** | Lightweight K8s by Canonical |
| **k3s** | Lightweight K8s by Rancher |

---

## ğŸ“ YAML Manifests

### Required Top-Level Fields

```yaml
apiVersion: v1          # API version (v1, apps/v1, etc.)
kind: Pod               # Object type (Pod, Deployment, Service)
metadata:               # Object metadata
  name: myapp
  labels:
    key: value
spec:                   # Desired state specification
  # Object-specific configuration
```

### Common API Versions

| Resource | API Version |
|:---------|:------------|
| Pod | `v1` |
| Service | `v1` |
| ConfigMap | `v1` |
| Secret | `v1` |
| Deployment | `apps/v1` |
| ReplicaSet | `apps/v1` |
| StatefulSet | `apps/v1` |
| DaemonSet | `apps/v1` |
| Ingress | `networking.k8s.io/v1` |

---

## ğŸ“‹ Commands Reference

### Cluster Commands

```bash
kubectl cluster-info                    # Cluster information
kubectl get nodes                       # List nodes
kubectl get all                         # All resources
kubectl api-resources                   # Available resource types
```

### Pod Commands

```bash
kubectl get pods                        # List pods
kubectl get pods -o wide                # Detailed list
kubectl get pods -w                     # Watch mode
kubectl describe pod <name>             # Pod details
kubectl logs <pod-name>                 # Pod logs
kubectl logs -f <pod-name>              # Follow logs
kubectl exec -it <pod> -- /bin/bash     # Shell into pod
kubectl delete pod <name>               # Delete pod
```

### Deployment Commands

```bash
kubectl get deployments                 # List deployments
kubectl get deploy                      # Short form
kubectl describe deployment <name>      # Details
kubectl create deployment <name> --image=<image> --replicas=3
kubectl set image deployment/<name> <container>=<image>
kubectl rollout status deployment/<name>
kubectl rollout history deployment/<name>
kubectl rollout undo deployment/<name>
kubectl rollout undo deployment/<name> --to-revision=<n>
kubectl scale deployment <name> --replicas=5
```

### ReplicaSet Commands

```bash
kubectl get replicaset                  # List ReplicaSets
kubectl get rs                          # Short form
kubectl describe rs <name>              # Details
kubectl scale rs <name> --replicas=5    # Scale
kubectl delete rs <name>                # Delete
```

### Service Commands

```bash
kubectl get services                    # List services
kubectl get svc                         # Short form
kubectl describe svc <name>             # Details
kubectl expose deployment <name> --type=NodePort --port=80
kubectl delete svc <name>               # Delete
```

### YAML Operations

```bash
kubectl create -f file.yaml             # Create from file
kubectl apply -f file.yaml              # Create or update
kubectl delete -f file.yaml             # Delete from file
kubectl replace -f file.yaml            # Replace resource
kubectl diff -f file.yaml               # Preview changes

# Generate YAML (dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml
```

### Debugging

```bash
kubectl describe <resource> <name>      # Detailed info
kubectl logs <pod-name>                 # View logs
kubectl logs <pod-name> -c <container>  # Specific container
kubectl exec -it <pod> -- sh            # Shell access
kubectl get events                      # Cluster events
kubectl explain <resource>              # Documentation
```

---

## ğŸ“š Additional Resources

| Resource | Link |
|:---------|:-----|
| Kubernetes Docs | [kubernetes.io/docs](https://kubernetes.io/docs/) |
| kubectl Cheat Sheet | [kubernetes.io/docs/reference/kubectl/cheatsheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/) |
| Minikube | [minikube.sigs.k8s.io](https://minikube.sigs.k8s.io/) |
| CNCF | [cncf.io](https://www.cncf.io/) |

---

<div align="center">

## ğŸš€ Ready to Continue?

**[â† Back to Main](../README.md)** | **[Helm â†’](../Helm-Concept/README.md)** | **[Terraform â†’](../Terraform-Concept/README.md)**

---

*"Kubernetes is not about containers. It's about applications."* â€” Kelsey Hightower

</div>

```
Personal Notes
> Node are simple VM(EC2), inside nodes we have pods, inside pods we have containers, those containers running on top of docker images.
> Cluster is a set of nodes together.

two type of servers:
	master, worker

Master Node: watches over the nodes in cluster and responsible for actual orch container on the worker nodes,
 
Kubernetes Components:	API server, etcd, kubelet, container runtime, controller, scheduler.
	API Server: acts as froentend for k8'scheduler(Users, managment devices, CLI, > talk to the api server to intract with k8's clusters).
	
	etcd: is a key store, a distributed reliable key store used by kubernetes to store all data used to manage the cluster.
		ex: in a cluster we have multiple nodes and multiple master, it stores all that info on all the nodes in the cluster in distributed manner.
		responsible for implement locks within the cluster to ensure that there no conflit b/w the masters.
		
	Scheduler: responsible for distributing work or containers across multiple node, looks for newly created containers and assign them to nodes.
	
	Controllers: Brain behind the orch, responsible for noticing and responding when nodes, containers or end points goes down.
		make decision to bring up the new containers.
	
	Container Runtime: is the underlying software that is used to run containers(Docker).
	
	Kubelet: is the agent that runs on each node in the cluster, responsible for making sure that the containers are running on the nodes as expected.
	=====
	"Kubectl": Kube cmd tool/kuberctl/kube control/
			Used to deploy and manage application on a kubernetes cluster. 
			To get cluster info, to get status of other nodes in a cluster, and manage many other things.
	::CMD::
		kubectl run <> ==> to deploy an application on the cluster.
		kubectl cluster-info ==> used to view information about cluster
		kubectl get nodes ==> to list all the nodes part of cluster

:::Master vs Worker:::
	Master has Kube API Server, and that is what makes it a master,
	etcd: All info gather are store in key value pair on the master
	controller, scheduler, are also part of master

	Containers are hosted on worker, to run containers we need container run time installed and that's where the container falls, >> (Docker)
		alternatives: Rocker or Cryo
	Worker node have the kubelet agent that is responsible for intracting with master to provide health info worker node, carry out actions req by master.
	

==============================
options avaliable to build a kubernetes cluster:
	1. minikube, microK8s, Kubeadm ==> developer or learner choice,
	
		Kubeadm tool used to bootstrap and manage production-grade kubernetes clusters.
		
		Minikube bundles all of the diff comp(API Server, etcd, scheduler, controller, kubelet, container runtime) into a single image(ISO Image)
		provide us a pre-config single node kubernetes cluster.
		(Single node kubernetes cluster)
		to work minikube smoothly, 
			1. Hypervisor, 2. kubectl utility, 3. minikube 
		
	2. Hosted Solutions:
		GCP, AWS, Azure, IBM, 
===============================
:::PODS:::
Our ultimate aim is to deploy our application in the form of container on a set of machines that are configured as worker nodes in a cluster,
	kubernetes doesn't directly deploy containers in the worker nodes.
	Containers Encapsulated into a Kubernetes onjects know as PODS. Pod is a single instance of an application.
	Pod > Smallest object that can create in k8s.
	(New pod, new instance, new application container) 'single pod doesn't support multiple containers of same kind'.
	1:1 with pod and container, pod contains multiple container(App, helper container) but not allowed of same type(app, app).
	two containers in pods also directly communicate with each other because it is in same pod(i.e., same n/w) and share volumes as well.
	
	YAML files as inputs for the creation of objects such as pods, Replicasets, deployment, servies ...
	Top level(root level)(req) fields:
		API version, Kind, Metadata, spec,
	kubectl create -f <yaml_file>
	kubectl apply -f <yaml_file>
	kubectl get pods
	kubectl describe pod <pod_name>
===============================
:::Replication Controllers and Replicasets:::
	
Controllers: Brain behind the orch, processes that monitor k8 obj and respond.
what is replica and why need of replication controller?
	if one fails, user doesn't face bad exp, so replicas needed, 
	replications controller:
		helps us run "multiple instances of single pod" in "k8 cluster", >> high avaliability
		it ensure that specified no of pods are running at all times, even if it's just one or a hundred.
		create multiple pods to share the load accress them (load balancer and scaling)
		ex: let's take single pod serving a set of users, when user increase, we deploy additional pod to balance the load across two pods.
		if the demand futher increase, and if we were to run out of resources on the first node, we could deploy additional pods across the other nodes in the cluster.
		replication controller spans accorss multiple nodes in the cluster, it helps us balance the load accorss multiple pods on diff nodes
		as well as scale our application when the demand increases.
			
Replication controller VS replica set:
Same purpose but not same:)

(Rep controller)old techonology replace by replica set(Added some more features).

How to write Rep controller file?
as discured earlier, Mainly 4 blocks: apiVersion, kind: ReplicationController, metadata, spec 
spec is most inportant
main task is to create multiple instance of pod, but what pod? so create a template section under spec to provide a pod details.
now ho many replicas, so add one more section in the spec as "replicas" to define no of replicas 

How to write Replicasets file?
as discured earlier, Mainly 4 blocks: 
apiVersion: apps/v1, 
kind: replicaset, 
metadata, 
spec 
almost all same, but one new thing added in spec i.e., "selector"
	selector sections helps the replica set identify what pods fall under it.
	but why?
	because replicaset can also manage pods that were not created as part of the replicaset creation, by using labels
by using labels replicaset comes to those are the pod i need to take care(monitor) whenever it fails, will spin-up a new pod.

replicaset scale or update?

do changes and then kubectl replace -f <yaml_file>
or else
kubectl scale --replicas=10 -f replicaset
or else
kubectl sclae --replicas=10 replicaset(type) myap-replicaset(name)
===============================
:::Deployment:::
	deploy in instances we have to take of few things below
	
	what if new version arrived? we need rolling update (one-after other), 
	what if their is error i new version, we have to undo (rollback), 
	what if have to multiple chages to the env?(web server version, as well as scaling env, modify the resource allocations ...)
	we don't want to apply each achanges immediately after the cmd run, instead we would like to apply a pause to our env make the changes and then resume so that 
	all the changes are rolled out together.
	
All these capabilities are avaliable with the k8 deployments.
	
Deep dive into update and rollback :
rollout and versioning in a deployment>> 
when you first create a deployment, it triggers a rollout, 
new rollout creates a new deploymant revision, in furture when app is upgrader, meaning when the container version is updated to new one, 
a new rollout is triggered and a new deploymant revision is created named revision 2,(helps track of deployment and enables us to rollback).

Deployment Strategies::
	Recreate || Rollingupdate(By default)
Recraete: delete(Destroy) all the pods and then spin-up new pods (Not efficient(not good pratice), we can see app down time)
	if their is some problem in new version untill it get fixed their is no pods and no application running,  
Rolling Update(Default): will delete some and spin-up some, and then delete some and then again spin-up some, no down time of app 
	if their is some problem in new version only first few pods get down and remaning all are in the running state (No down time) 
	whenever problem resolve and get spin-up new ones then only the remaning goes for update
	
How Upgrade works::
**Imp: It create a new replicaset automatically**
in old replica delete x no of pods and in new replica added x no of pods, 
simillarly does the process untill it reaches the desired value of pod in replicaset defined in yaml file,
if you undo the upgrade then same principle works


To upadte we have many options
1. do all the necessary changes in dep.yaml file and apply this command
kubectl apply -f <dep.yaml>

2.	kubectl set image <dep_name> <conatiner_name=image:tag>

kubectl describe deployment <name_dep>

kubectl rollout undo <dep_name>
===============================
::Networking in k8s::
	>Node has IP address, Usecase: access the k8 node, SSH into it,
	> IP address is assigned to a pod. Each pod in k8 gets its own internal IP with a series (ex: 10.244.0.2, 10.244.0.3, 10.244.0.4)
	why and how this 10.244.X.X series, when k8 initially config, we create an internal private network with address (i.e., 10.244.0.0) and all the pods are attached to it.
	when you deploy multiple pods, they all get a seperate IP assigned from this n/w.
	The pods can communicate with each other through this IP. But accessing the other pods using this internal IP addree may not be a good idea, as its sub to changes when pods are recreated.
	
	Multiple Nodes:
	if two nodes running k8 and they have IP address(not a part of the cluster yet), Each of them has a single pod deployed, these pods are attached to internal network and they have ther own IP address assigned.
	problem arise:
		 same n/w and same IP causes conflit. k8 dosen't automatically setup any kind of n/w to handle these issues.
	so we need to setup n/w to meet certain fund,
		All containers/PODs can communicateto one another without NAT,
		All nodes can communicate with all containers and vice-verca without NAT.
		pre-build solutions:-
			CISCO ACI n/w, Cilium, flannel, VMware NSX-T, Big cloud fabric, calico, 
			if scratch > calico/fannel is best, k8 labs weave net
		by using the above, it now manages the n/w and IPs in my nodes and assign a diff n/w add for each n/w in node.
	This crate a virtual n/w of all pods and nodes where they are all assigned a unique IP address, 
	by using simple routing techniques the cluster n/w enables communication b/w the different pods and nodes
===============================
:::Services in k8's:::
Enables communication b/w various comp within and outside of the app,
help us to connect app together with other apps or users
ex:- services enable the frontend app tot he user, 
	helps communication b/w backend and frontend pods, 
	and helps establishing connectivity to the external data source(database)
	services enables loose coupling b/w microservices in our app
ex:-
i have laptop ip(192.168.1.10) in my laptop i have a node ip as (192.168.1.2) inside node we have a private n/w of ip as(10.244.0.0)
our pod is runing on internal n/w so our pod ip is series of 10.244.X.X for instance (10.244.0.2) which runing my froentend application
Now i want to see my app in my laptop browser
how?
1st we need to SSH to k8 node from our laptop and from the node we would be access to access the pod's web page by doing curl or GUI
reason: Both are in same N/w(Node and pod) but it is inside the k8 node, 
"not good pratice", i want to hit the laptop ip and want to see the runing app,
now services comes into the picture, 
it is like a object in k8 node, just simply forming node forwarding from external(listen) to internal(forward)

Services types::
Nodeport: where the service makes an internal pod accessible on a port on the node.
ClusterIP: create a virtual ip inside the cluster to enab;e the comm b/w different services such as set of frontend/backend servers.
Loadbalancer: it provision a load balancer to the app in support cloud providers(distribute load accross diff web servers)

Let's depp dive into one after other
Nodeport: 
mapping a port on the node to a port on the pod, 
the port on the pod actual web server is running, ie., (80)and it refer to target port, 
service is like a virtual server inside the node, inside cluseter it has its own IP address called clusterIP of the service. 
and finally we have a port on node itself, which we use access to webserver externally(nodeport) ie., 30008 (valid range 30,000 to 32,767)
curl htp://XXXXXXX:30008
How to write a file?
as similar to above files, 4 sections, 
apiVersion: v1
kind: service
metadata:
spec: 
only difference b/w we can see in spec section we have type=nodeport and ports and have to define selector to identify the pods

what if we have multiple pods??
by using selector and label, service automatically connects all the node that is same label, uses random algo to disrtibute the traffic

what if we have multiple nods??
k8's automatically expands services to all the nodes in cluster and maps the target port to same node port on all the nods in the cluster.
can access app using any ip address but same port number 30008, 

ClusterIP:
ex: a full stack app having fronetend, backend, database
need to communicate each other? and if the old pods goes down, create new pod with different IP, so it's hard to commmunicate with IP address as discussed in previous(Nodeport)
so lets create a service cluster ip for all the frontend, simillarly create clusterIP for all the backend, and simillarly for all the databases, 
(no need to take care about individual ip-address)automatically service will take care about it by using selector and label, just we need to create a service of ClusterIP for each set of types of pods(frontend, backend, databases)
apiVersion: v1
kind: service
metadata:
spec: 
only difference b/w we can see in spec section we have type=ClusterIP and ports and have to define selector to identify the pods

::NOTE::
let's for example we have 4 nodes, out of these we have two node running voting-app with 1pod and 2 pods
and other two are running results app with 1 pod and 2 pods, 
now we created a 2 service one for voting and one for results, 
we are able to access the voting node ip to the results app by using it's portnumber (because service is created all over) simillary vice-versa
 

Loadbalancer:

===============================
:::K8's in cloud:::

Self Hosted:
	provision & Configure VMs, Use script to deploy cluster, maintain vm ourself, 
	Ex: Kubernetes on AWS using kops/ KubeOne.
Hosted Solutions:
	K8-as-a-service, Providers provision vm's / install K8's / maintain vm's.
	ex: GKE(Google Kubernates Engine), Azure Kubernetes Service(AKS), Amazon Elastic kubernetes Service(EKS).

===============================
>>YAML file is used to represent configuration data.

:::CMD::: 
kubectl get all >> to get all obj of k8
kubectl run <> ==> to deploy an application on the cluster.
kubectl cluster-info ==> used to view information about cluster
kubectl get nodes ==> to list all the nodes part of cluster
kubectl get nodes
kubectl create deployment hello-minikube --image=k8s.grc.io/echoserver:1.10 (Refer doc: <<https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download>>)
kubectl get deployment
kubectl expose deployment hello-minikube --type=NodePort --port=8080
minikube service hello-minikube --url
kubectl run ngx --image nginx ==> created 
kubectl describe pod <ngx>



kubectl create -f <yaml_file>
kubectl apply -f <yaml_file>
kubectl get pods
kubectl get po
kubectl get pods -o wide
kubectl describe pod <pod_name>

kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml
kubectl edit pod <pod_name>
kubectl replace -f <yaml_file> >> did any change in yaml afile nd want to apply the changes then use this cmd.
Diff?
kubectl run ngx --image nginx ==> created 
kubectl create deployment ngx --image=nginx 

kubectl explian replicaset
kubectl get replicationcontroller
kubectl get replicaset
kubectl get rs
kubectl describe replicaset 
kubectl delete replicaset
kubectl replace -f <replace.yaml>
kubectl scale --replicas=10 -f replicaset
kubectl sclae --replicas=10 replicaset(type) myap-replicaset(name)
kubectl api-resources | grep replicaset
kubectl explain replicaset | grep VERSION
kubectl delete replicaset <name>
kubectl edit replicaset <name>
kubectl scale replicaset <name> --replicas=2

kubectl create -f deployment.yaml
kubectl get deployments
kubectl get replicaset
kubectl get pods
kubectl create deployment "abc" --image:"im" --replicas=3

kubectl cetare -f deploy.yaml
kubectl get deploy
kubectl apply -f dep.yaml
kubectl rollout status <name_deployment> >>
kubectl rollout history <name_deployment>
1. do all the necessary changes in dep.yaml file and apply this command
kubectl apply -f <dep.yaml>
2.	kubectl set image <dep_name> <conatiner_name=image:tag>

kubectl describe deployment <name_dep>

kubectl rollout undo <dep_name>


kubectl create -f service.yaml
kubectl get services



:::Hiraricy:::
Image >> Container >> Pods >> Replicaset >> Deployment >>
```
